{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar - ML + SoC \n",
    "### Based on the training \"From Algorithm to Hardware: Machine Learning in Embedded Systems\"\n",
    "\n",
    "##### San Luis, Argentina - 2025\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Synthesis for Machine Learning (hls4ml)\n",
    "\n",
    "\n",
    "ðŸ’¡ **High-Level Synthesis for Machine Learning (hls4ml)**  is an open-source library that transforms machine learning models into hardware descriptions optimized for FPGA deployment.\n",
    "\n",
    "**Key Features of hls4ml:** \n",
    "\n",
    "- Converts models from Keras, TensorFlow, PyTorch, and ONNX into High-Level Synthesis (HLS) projects.\n",
    "\n",
    "- Utilizes tools like Xilinx Vitis HLS and Intel HLS Compiler to generate optimized C++ code for hardware implementation.\n",
    "\n",
    "- Enhances efficiency by reducing latency and power consumption, making it ideal for AI applications in edge computing.\n",
    "\n",
    "- Supports quantization and pruning techniques to shrink model size while maintaining accuracy.\n",
    "\n",
    "\n",
    "For further details:\n",
    "\n",
    "- GitHub: https://github.com/fastmachinelearning/hls4ml\n",
    "\n",
    "- Web site: https://fastmachinelearning.org/hls4ml/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibrerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 20:27:18.972028: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/tools/anaconda3/envs/neuralEnv/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/tools/anaconda3/envs/neuralEnv/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Unable to import optimizer(s) from expr_templates.py: No module named 'sympy'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from qkeras import *\n",
    "from qkeras import QActivation\n",
    "from qkeras import QDense, QConv2DBatchnorm\n",
    "import hls4ml\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path a Vitis HLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como paso inicial se debe especificar el directorio de instalaciÃ³n de Vivado HLS o Vitis HLS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tools/Xilinx/Vitis_HLS/2022.2/bin:/tools/Xilinx/XilinxUnified_2022/Vitis_HLS/2022.2/bin:/tools/anaconda3/envs/neuralEnv/bin:/tools/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path donde se encuentra instalado Vitis HLS!! \n",
    "\n",
    "os.environ['PATH'] = '/tools/Xilinx/XilinxUnified_2022/Vitis_HLS/2022.2/bin:' + os.environ['PATH']\n",
    "\n",
    "# Para MÃQUINA VIRTUAL!\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis_HLS/2022.2/bin:' + os.environ['PATH']\n",
    "os.environ['PATH']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model (.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 20:27:22.909568: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-04-14 20:27:22.910173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2025-04-14 20:27:23.003373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.003543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.155GHz coreCount: 14 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 149.04GiB/s\n",
      "2025-04-14 20:27:23.003570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-04-14 20:27:23.004981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-04-14 20:27:23.005061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-04-14 20:27:23.006369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-04-14 20:27:23.006620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-04-14 20:27:23.008062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-04-14 20:27:23.008903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-04-14 20:27:23.011974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-04-14 20:27:23.012153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.012394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.012533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2025-04-14 20:27:23.013144: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-14 20:27:23.013661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-04-14 20:27:23.013823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.013980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.155GHz coreCount: 14 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 149.04GiB/s\n",
      "2025-04-14 20:27:23.014006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-04-14 20:27:23.014045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-04-14 20:27:23.014056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-04-14 20:27:23.014067: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-04-14 20:27:23.014078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-04-14 20:27:23.014090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-04-14 20:27:23.014101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-04-14 20:27:23.014113: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-04-14 20:27:23.014181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.014327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.014427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2025-04-14 20:27:23.014457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-04-14 20:27:23.366072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-04-14 20:27:23.366099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2025-04-14 20:27:23.366104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2025-04-14 20:27:23.366323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.366452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.366542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-14 20:27:23.366625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3152 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc1_input (QDense)           (None, 5)                 3925      \n",
      "_________________________________________________________________\n",
      "relu_input (QActivation)     (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "fc1 (QDense)                 (None, 7)                 42        \n",
      "_________________________________________________________________\n",
      "relu1 (QActivation)          (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "fc2 (QDense)                 (None, 10)                80        \n",
      "_________________________________________________________________\n",
      "relu2 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "output (QDense)              (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "sigmoid (Activation)         (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,069\n",
      "Trainable params: 4,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "model = load_model('models/mnistQAP.h5', custom_objects=co)\n",
    "    \n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weight distribution\n",
    "\n",
    "weights = np.concatenate([w.flatten() for w in model.get_weights()])\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.hist(weights, bins=60, color='green', alpha=0.6)\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Model MLP for MNIST - Weight Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high-Level Synthesis for Machine Learning (hls4ml )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hls configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input_input, layer type: InputLayer, input shapes: [[None, 784]], output shape: [None, 784]\n",
      "Layer name: fc1_input, layer type: QDense, input shapes: [[None, 784]], output shape: [None, 5]\n",
      "Layer name: relu_input, layer type: Activation, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 7]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 7]], output shape: [None, 7]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 7]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 2]\n",
      "Layer name: sigmoid, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "-----------------------------------\n",
      "Model\n",
      "  Precision:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  fc1_input_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  fc1_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,4>\n",
      "      bias:          fixed<8,4>\n",
      "  fc1_input_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu_input\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,7,RND_CONV,SAT>\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,4>\n",
      "      bias:          fixed<8,4>\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,7,RND_CONV,SAT>\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,4>\n",
      "      bias:          fixed<8,4>\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,7,RND_CONV,SAT>\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "      weight:        fixed<8,4>\n",
      "      bias:          fixed<8,4>\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "  sigmoid\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<16,6>\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(layers=['Activation'])\n",
    "hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(rounding_mode='AP_RND')\n",
    "hls4ml.model.optimizer.get_optimizer('output_rounding_saturation_mode').configure(saturation_mode='AP_SAT')\n",
    "\n",
    "\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "import plotting\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(hls_config)\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][layer]['Trace'] = True\n",
    "    hls_config['LayerName'][layer]['ReuseFactor'] = 16\n",
    "\n",
    "hls_config['LayerName']['fc1_input_input']['Precision'] = 'ap_fixed<16, 6>'   \n",
    "hls_config['LayerName']['sigmoid']['Strategy'] = 'Stable'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hls4ml with Vitis HLS as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: fc1_input_input, layer type: InputLayer, input shapes: [[None, 784]], output shape: [None, 784]\n",
      "Layer name: fc1_input, layer type: QDense, input shapes: [[None, 784]], output shape: [None, 5]\n",
      "Layer name: relu_input, layer type: Activation, input shapes: [[None, 5]], output shape: [None, 5]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 5]], output shape: [None, 7]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 7]], output shape: [None, 7]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 7]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 2]\n",
      "Layer name: sigmoid, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Creating HLS model\n"
     ]
    }
   ],
   "source": [
    "cfg = hls4ml.converters.create_config(backend='vitis')\n",
    "\n",
    "# cfg['IOType']     = 'io_stream'   # Must set this if using CNNs!\n",
    "cfg['HLSConfig']  = hls_config      # HLS configuraiton\n",
    "cfg['KerasModel'] = model           # Keras model to be converted\n",
    "cfg['OutputDir']  = 'hls4ml/'       # Project name\n",
    "cfg['Part'] = 'xc7z020clg484-1'     # PYNQ-Z1 or Zedboard: xc7z020clg484-1  ARTIX-7 xc7a35tcsg325-1  # MPSoC xczu4eg-sfvc784-2-e  xczu3eg-sfvc784-1-e\n",
    "\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardware synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vitis HLS - High-Level Synthesis from C, C++ and OpenCL v2022.2 (64-bit)\n",
      "  **** SW Build 3670227 on Oct 13 2022\n",
      "  **** IP Build 3669848 on Fri Oct 14 08:30:02 MDT 2022\n",
      "    ** Copyright 1986-2022 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /tools/Xilinx/XilinxUnified_2022/Vitis_HLS/2022.2/scripts/vitis_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/tools/Xilinx/XilinxUnified_2022/Vitis_HLS/2022.2/bin/unwrapped/lnx64.o/vitis_hls'\n",
      "INFO: [HLS 200-10] For user 'ro' on host 'mareKaleido' (Linux_x86_64 version 5.15.0-136-generic) on Mon Apr 14 20:27:56 CEST 2025\n",
      "INFO: [HLS 200-10] On os Ubuntu 20.04.6 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/ro/kaleido/repo/seminar-ML-FPGA-SanLuis2025/hands-on/hls4ml'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-1510] Running: open_project myproject_prj \n",
      "INFO: [HLS 200-10] Creating and opening project '/home/ro/kaleido/repo/seminar-ML-FPGA-SanLuis2025/hands-on/hls4ml/myproject_prj'.\n",
      "INFO: [HLS 200-1510] Running: set_top myproject \n",
      "INFO: [HLS 200-1510] Running: add_files firmware/myproject.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb myproject_test.cpp -cflags -std=c++0x \n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb firmware/weights \n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-1510] Running: add_files -tb tb_data \n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-1510] Running: open_solution solution1 \n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/ro/kaleido/repo/seminar-ML-FPGA-SanLuis2025/hands-on/hls4ml/myproject_prj/solution1'.\n",
      "INFO: [HLS 200-1505] Using default flow_target 'vivado'\n",
      "Resolution: For help on HLS 200-1505 see www.xilinx.com/cgi-bin/docs/rdoc?v=2022.2;t=hls+guidance;d=200-1505.html\n",
      "INFO: [HLS 200-435] Setting 'open_solution -flow_target vivado' configuration: config_interface -m_axi_latency=0\n",
      "INFO: [HLS 200-1510] Running: config_array_partition -maximum_size 4096 \n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "ERROR: [HLS 200-642] The 'config_array_partition -maximum_size' command is not supported.\n",
      "INFO: [HLS 200-1510] Running: config_compile -name_max_length 80 \n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: set_part xc7z020clg484-1 \n",
      "INFO: [HLS 200-1611] Setting target device to 'xc7z020-clg484-1'\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set to 80.\n",
      "INFO: [HLS 200-1510] Running: config_schedule -enable_dsp_full_reg=false \n",
      "INFO: [HLS 200-1510] Running: create_clock -period 5 -name default \n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [HLS 200-1510] Running: set_clock_uncertainty 12.5% default \n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [HLS 200-1510] Running: csynth_design \n",
      "Running Dispatch Server on port: 40345\n",
      "INFO: [HLS 200-111] Finished File checks and directory preparation: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 10.01 seconds; current allocated memory: 224.848 MB.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "WARNING: [HLS 207-5292] unused parameter 'keep' (firmware/nnet_utils/nnet_helpers.h:285:99)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_code_gen.h:11:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_code_gen.h:12:36)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_code_gen.h:13:44)\n",
      "WARNING: [HLS 207-5292] unused parameter 'data' (firmware/nnet_utils/nnet_code_gen.h:21:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'buffer' (firmware/nnet_utils/nnet_code_gen.h:22:24)\n",
      "WARNING: [HLS 207-5292] unused parameter 'partition' (firmware/nnet_utils/nnet_code_gen.h:23:32)\n",
      "INFO: [HLS 200-111] Finished Source Code Analysis and Preprocessing: CPU user time: 6.7 seconds. CPU system time: 0.43 seconds. Elapsed time: 7.6 seconds; current allocated memory: 230.117 MB.\n",
      "INFO: [HLS 200-777] Using interface defaults for 'Vivado' flow target.\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:42:27)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config5::weight_t*, config5::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:42:27)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:42:27)\n",
      "INFO: [HLS 214-131] Inlining function 'nnet::product::mult<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0> >::product(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 4, (ap_q_mode)5, (ap_o_mode)3, 0>)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config11::weight_t*, config11::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:42:27)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' into 'myproject(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>*)' (firmware/myproject.cpp:41:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config5::weight_t*, config5::bias_t*)' into 'myproject(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>*)' (firmware/myproject.cpp:53:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' into 'myproject(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>*)' (firmware/myproject.cpp:65:2)\n",
      "INFO: [HLS 214-131] Inlining function 'void nnet::dense<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config11::weight_t*, config11::bias_t*)' into 'myproject(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>*)' (firmware/myproject.cpp:77:2)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:79:15)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:75:15)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:71:15)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:67:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:63:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:59:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:55:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:51:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:47:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:43:14)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/myproject.cpp:39:11)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_114_1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:114:23)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:99:32)\n",
      "INFO: [HLS 214-291] Loop 'VITIS_LOOP_31_1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_activation.h:31:19)\n",
      "INFO: [HLS 214-291] Loop 'Result' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:64:5)\n",
      "INFO: [HLS 214-291] Loop 'Accum1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:54:5)\n",
      "INFO: [HLS 214-291] Loop 'Accum2' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:56:9)\n",
      "INFO: [HLS 214-291] Loop 'ResetAccum' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:48:5)\n",
      "INFO: [HLS 214-291] Loop 'Product1' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:37:5)\n",
      "INFO: [HLS 214-291] Loop 'Product2' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:40:9)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:18:32)\n",
      "INFO: [HLS 214-291] Loop 'anonymous' is marked as complete unroll implied by the pipeline pragma (firmware/nnet_utils/nnet_dense_latency.h:17:32)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:79:15) in function 'myproject' completely with a factor of 2 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:75:15) in function 'myproject' completely with a factor of 2 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:71:15) in function 'myproject' completely with a factor of 10 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:67:14) in function 'myproject' completely with a factor of 10 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:63:14) in function 'myproject' completely with a factor of 10 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:59:14) in function 'myproject' completely with a factor of 7 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:55:14) in function 'myproject' completely with a factor of 7 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:51:14) in function 'myproject' completely with a factor of 7 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:47:14) in function 'myproject' completely with a factor of 5 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:43:14) in function 'myproject' completely with a factor of 5 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/myproject.cpp:39:11) in function 'myproject' completely with a factor of 5 (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_114_1' (firmware/nnet_utils/nnet_activation.h:114:23) in function 'nnet::sigmoid<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, sigmoid_config13>' completely with a factor of 2 (firmware/nnet_utils/nnet_activation.h:95:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_activation.h:99:32) in function 'nnet::sigmoid<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, sigmoid_config13>' completely with a factor of 1024 (firmware/nnet_utils/nnet_activation.h:95:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config12>' completely with a factor of 2 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:18:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 2 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:17:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' completely with a factor of 20 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config10>' completely with a factor of 10 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config9>' completely with a factor of 10 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:18:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 10 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:17:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' completely with a factor of 70 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config7>' completely with a factor of 7 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config6>' completely with a factor of 7 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37:5) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40:9) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:18:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 7 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:17:32) in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' completely with a factor of 35 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config4>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'VITIS_LOOP_31_1' (firmware/nnet_utils/nnet_activation.h:31:19) in function 'nnet::linear<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, linear_config3>' completely with a factor of 5 (firmware/nnet_utils/nnet_activation.h:28:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64:5) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54:5) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 784 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56:9) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48:5) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37:5) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 784 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40:9) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:18:32) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 5 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-186] Unrolling loop 'anonymous' (firmware/nnet_utils/nnet_dense_latency.h:17:32) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 3920 (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_uint<1> >::value), ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(config2::accum_t)' into 'void nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>(ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config2::weight_t*, config2::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>(config5::accum_t)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config5::weight_t*, config5::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(config8::accum_t)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config8::weight_t*, config8::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-178] Inlining function 'std::enable_if<!(std::is_same<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_uint<1> >::value), ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::type nnet::cast<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>(config11::accum_t)' into 'void nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>(ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>*, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>*, config11::weight_t*, config11::bias_t*)' (firmware/nnet_utils/nnet_dense_latency.h:15:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b11': Complete partitioning on dimension 1. (firmware/weights/b11.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b8': Complete partitioning on dimension 1. (firmware/weights/b8.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b5': Complete partitioning on dimension 1. (firmware/weights/b5.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'b2': Complete partitioning on dimension 1. (firmware/weights/b2.h:12:0)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'mult': Complete partitioning on dimension 1. (firmware/nnet_utils/nnet_dense_latency.h:17:32)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer2_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:39:11)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer3_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:43:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer4_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:47:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer5_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:51:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer6_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:55:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer7_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:59:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer8_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:63:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer9_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:67:14)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer10_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:71:15)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer11_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:75:15)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer12_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:79:15)\n",
      "INFO: [HLS 214-248] Applying array_partition to 'layer13_out': Complete partitioning on dimension 1. (firmware/myproject.cpp:9:0)\n",
      "WARNING: [HLS 214-292] Unsupported reshape pragma/directive on variable 'fc1_input_input'. The port size might be shrunk or fail cosim as the bit-width after reshape (12544) is larger than 4096\n",
      "INFO: [HLS 214-248] Applying array_reshape to 'fc1_input_input': Complete reshaping on dimension 1. (firmware/myproject.cpp:9:0)\n",
      "INFO: [HLS 200-111] Finished Compiling Optimization and Transform: CPU user time: 275.41 seconds. CPU system time: 0.83 seconds. Elapsed time: 276.61 seconds; current allocated memory: 250.828 MB.\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas: CPU user time: 0 seconds. CPU system time: 0 seconds. Elapsed time: 0 seconds; current allocated memory: 250.828 MB.\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-111] Finished Standard Transforms: CPU user time: 0.78 seconds. CPU system time: 0.04 seconds. Elapsed time: 0.88 seconds; current allocated memory: 384.887 MB.\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability: CPU user time: 0.91 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.92 seconds; current allocated memory: 485.820 MB.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation.h:109:9) to (firmware/nnet_utils/nnet_activation.h:123:1) in function 'nnet::sigmoid<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, sigmoid_config13>'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config8>' (firmware/nnet_utils/nnet_dense_latency.h:33:1)...46 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:33:1)...17 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 7, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config11>' (firmware/nnet_utils/nnet_dense_latency.h:33:1)...14 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:33:1)...1944 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Loop, function and other optimizations: CPU user time: 1.84 seconds. CPU system time: 0.05 seconds. Elapsed time: 1.9 seconds; current allocated memory: 635.934 MB.\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis: CPU user time: 2.82 seconds. CPU system time: 0.03 seconds. Elapsed time: 2.84 seconds; current allocated memory: 839.977 MB.\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config2>' to 'dense_latency_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config3>' to 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config4>' to 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config4_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config5>' to 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config6>' to 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config7>' to 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config7_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config8>' to 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config9>' to 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config10>' to 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config10_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>' to 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config11_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config12>' to 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config12_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'sigmoid<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 6, 0, 0, 0>, sigmoid_config13>' to 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s'.\n",
      "WARNING: [SYN 201-223] Checking resource limit in 'dense_latency<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config2>': cannot find any operation of 'mul'.\n",
      "WARNING: [SYN 201-223] Checking resource limit in 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>': cannot find any operation of 'mul'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config2>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 8, Depth = 8, function 'dense_latency<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 5, 3, 0>, config2>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 4.34 seconds. CPU system time: 0.05 seconds. Elapsed time: 4.41 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 8.56 seconds. CPU system time: 0.02 seconds. Elapsed time: 8.58 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config3>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config3>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 5.9 seconds. CPU system time: 0.02 seconds. Elapsed time: 5.97 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config4>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config4>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config5>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 4, Depth = 4, function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config5>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.09 seconds. CPU system time: 0 seconds. Elapsed time: 0.09 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.07 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config6>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config6>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.12 seconds. CPU system time: 0 seconds. Elapsed time: 0.12 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config7>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config7>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.02 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config8>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 4, Depth = 4, function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config8>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.15 seconds. CPU system time: 0 seconds. Elapsed time: 0.15 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.12 seconds. CPU system time: 0 seconds. Elapsed time: 0.12 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config9>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config9>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.21 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.22 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.02 seconds. CPU system time: 0 seconds. Elapsed time: 0.01 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config10>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 7, 0, 0, 0>, linear_config10>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.03 seconds. CPU system time: 0 seconds. Elapsed time: 0.03 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 4, Depth = 4, function 'dense_latency<ap_fixed<16, 7, 0, 0, 0>, ap_fixed<16, 6, 5, 3, 0>, config11>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.08 seconds. CPU system time: 0 seconds. Elapsed time: 0.08 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config12>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 1, function 'linear<ap_fixed<16, 6, 5, 3, 0>, ap_fixed<16, 6, 0, 0, 0>, linear_config12>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.09 seconds. CPU system time: 0 seconds. Elapsed time: 0.09 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.01 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.01 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'sigmoid<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 6, 0, 0, 0>, sigmoid_config13>'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 1, Depth = 4, function 'sigmoid<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 6, 0, 0, 0>, sigmoid_config13>'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.04 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'myproject'.\n",
      "INFO: [HLS 200-1470] Pipelining result : Target II = NA, Final II = 8, Depth = 27, function 'myproject'\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111] Finished Scheduling: CPU user time: 0.11 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.11 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111] Finished Binding: CPU user time: 1.66 seconds. CPU system time: 0 seconds. Elapsed time: 1.67 seconds; current allocated memory: 910.250 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_latency_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config2_s' is 12754 from HDL expression: (1'b1 == ap_CS_fsm_state1)\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_16_6_5_3_0_ap_fixed_16_6_5_3_0_config2_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 8.93 seconds. CPU system time: 0.04 seconds. Elapsed time: 9.04 seconds; current allocated memory: 980.242 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config3_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 16.08 seconds. CPU system time: 0.27 seconds. Elapsed time: 16.41 seconds; current allocated memory: 1.066 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config4_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.05 seconds; current allocated memory: 1.081 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_16s_5ns_19_2_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_16s_5s_19_2_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_16s_6s_19_2_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config5_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.1 seconds. CPU system time: 0 seconds. Elapsed time: 0.1 seconds; current allocated memory: 1.081 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config6_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.27 seconds. CPU system time: 0.02 seconds. Elapsed time: 0.29 seconds; current allocated memory: 1.083 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config7_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.04 seconds. CPU system time: 0 seconds. Elapsed time: 0.05 seconds; current allocated memory: 1.085 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_16s_5ns_19_2_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'mul_16s_5s_19_2_1': 2 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config8_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.11 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.13 seconds; current allocated memory: 1.086 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config9_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.53 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.8 seconds; current allocated memory: 1.087 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_0_0_0_ap_fixed_16_7_0_0_0_linear_config10_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0 seconds. Elapsed time: 0.27 seconds; current allocated memory: 1.094 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_16_7_0_0_0_ap_fixed_16_6_5_3_0_config11_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.1 seconds. CPU system time: 0 seconds. Elapsed time: 0.4 seconds; current allocated memory: 1.095 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_ap_fixed_16_6_5_3_0_ap_fixed_16_6_0_0_0_linear_config12_s'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.2 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.47 seconds; current allocated memory: 1.097 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s_sigmoid_table_ROM_AUTO_1R' to 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s_sigmoid_tabkb' due to the length limit 80\n",
      "INFO: [HLS 200-1030] Apply Unified Pipeline Control on module 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s' pipeline 'sigmoid<ap_fixed<16, 6, 0, 0, 0>, ap_fixed<16, 6, 0, 0, 0>, sigmoid_config13>' pipeline type 'function pipeline'\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s'.\n",
      "INFO: [RTMG 210-279] Implementing memory 'myproject_sigmoid_ap_fixed_16_6_0_0_0_ap_fixed_16_6_0_0_0_sigmoid_config13_s_sigmoid_tabkb' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.05 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.3 seconds; current allocated memory: 1.098 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/fc1_input_input' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer13_out_0' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer13_out_1' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [HLS 200-1030] Apply Unified Pipeline Control on module 'myproject' pipeline 'myproject' pipeline type 'function pipeline'\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'myproject' is 12562 from HDL expression: ap_rst\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [HLS 200-111] Finished Creating RTL model: CPU user time: 0.18 seconds. CPU system time: 0.01 seconds. Elapsed time: 0.51 seconds; current allocated memory: 1.099 GB.\n",
      "INFO: [HLS 200-111] Finished Generating all RTL models: CPU user time: 8.87 seconds. CPU system time: 0.01 seconds. Elapsed time: 8.9 seconds; current allocated memory: 1.103 GB.\n",
      "INFO: [HLS 200-111] Finished Updating report files: CPU user time: 0.86 seconds. CPU system time: 0.03 seconds. Elapsed time: 0.93 seconds; current allocated memory: 1.119 GB.\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 232.43 MHz\n",
      "INFO: [HLS 200-111] Finished Command csynth_design CPU user time: 346.82 seconds. CPU system time: 1.95 seconds. Elapsed time: 351.66 seconds; current allocated memory: 921.430 MB.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h6m1s *****\n",
      "INFO: [HLS 200-112] Total CPU user time: 348.02 seconds. Total CPU system time: 2.11 seconds. Total elapsed time: 363.54 seconds; peak allocated memory: 1.119 GB.\n",
      "INFO: [Common 17-206] Exiting vitis_hls at Mon Apr 14 20:33:59 2025...\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n",
       "  'EstimatedClockPeriod': '4.302',\n",
       "  'BestLatency': '26',\n",
       "  'WorstLatency': '26',\n",
       "  'IntervalMin': '8',\n",
       "  'IntervalMax': '8',\n",
       "  'BRAM_18K': '1',\n",
       "  'DSP': '6',\n",
       "  'FF': '37553',\n",
       "  'LUT': '54132',\n",
       "  'URAM': '0',\n",
       "  'AvailableBRAM_18K': '280',\n",
       "  'AvailableDSP': '220',\n",
       "  'AvailableFF': '106400',\n",
       "  'AvailableLUT': '53200',\n",
       "  'AvailableURAM': '0'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.build(csim=False, export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### San Luis, Argentina - 2025\n",
    "\n",
    "Romina Soledad Molina, Ph.D. - MLab/STI ICTP, Trieste, Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
